{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "47c390eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "loading config\n",
    "'''\n",
    "\n",
    "from config import (\n",
    "    DATA_DIR, METADATA_PATH,\n",
    "    OUTPUT_DIR, VISUALS_DIR, CSV_DIR,\n",
    "    NUM_TOPICS, MAX_DF, MIN_DF\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "00427b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "loading text chunks\n",
    "'''\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for file in sorted(DATA_DIR.glob(\"*.txt\")):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts.append(f.read())\n",
    "        filenames.append(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed3572",
   "metadata": {},
   "source": [
    "#### spaCy\n",
    "The following script was used to generate tokens needed for the final topic modeling. \n",
    "Certain words like \"herr\", \"hand\", or persons' names like \"Klamm\" (the famous beaurocrat of the novel \"the castle\")\n",
    "were seen in the topics words that did not halp getting any insight into the topics. \n",
    "Also verbs are filtered out in the script to avoid repititions in the topic words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74221980",
   "metadata": {},
   "source": [
    "#### tokenizer\n",
    "other models like \"de_core_news_md\"  and \"de_core_news_lg\" were also experimented but the final one\n",
    "seen in the script below yielded in the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#nlp = spacy.load(\"de_core_news_sm\")\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "# using common stop-words to filter them out\n",
    "CUSTOM_STOPWORDS = {\n",
    "    \"hand\", \"herr\", \"delamarch\", \"pollunder\", \"auge\", \"klamm\", \"frau\", \"gesicht\", \"mann\", \"sehen\",\n",
    "    \"fragen\", \"wissen\", \"barnabas\", \"sagen\", \"stehen\", \"gehen\", \"kommen\", \"zimmer\",\n",
    "}\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([\n",
    "        lemma for token in doc\n",
    "        if token.pos_ in [\"NOUN\", \"ADJ\"]\n",
    "        and not token.is_stop\n",
    "        and token.is_alpha\n",
    "        and len(token.lemma_) > 3\n",
    "        and (lemma := token.lemma_.lower()) not in CUSTOM_STOPWORDS\n",
    "    ])\n",
    "\n",
    "texts_cleaned = [preprocess(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "029dd80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "save cleaned texts to CSV\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_cleaned = pd.DataFrame({\n",
    "    \"filename\": filenames,\n",
    "    \"text\": texts_cleaned\n",
    "})\n",
    "\n",
    "df_cleaned.to_csv(CSV_DIR / \"cleaned_chunks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170daca",
   "metadata": {},
   "source": [
    "#### A broader preprocessing\n",
    "The following script is for a broader tokenization of texts. Verbs, nouns and adjectives are taken into account,\n",
    "but it results in many VERBS in the topic word lists. Hence another approach was taken by filtering\n",
    "out the verbs and common words. \n",
    "This script can be run to compare the topic words with the filtering method used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348688d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Preprocessing using spaCy (broader filtering)\n",
    "'''\n",
    "import spacy\n",
    "\n",
    "# loading German spacy model\n",
    "nlp = spacy.load(\"de_core_news_sm\")  # or \"de_core_news_md\" / \"de_core_news_lg\" or \"de_core_news_sm\"\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([\n",
    "        token.lemma_.lower()\n",
    "        for token in doc\n",
    "        #if token.pos_ in [\"NOUN\"]\n",
    "        if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\"]\n",
    "        and not token.is_stop\n",
    "        and token.is_alpha\n",
    "    ])\n",
    "\n",
    "texts_cleaned = [preprocess(text) for text in texts]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka_topic_modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
