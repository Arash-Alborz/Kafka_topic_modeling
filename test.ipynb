{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edc5602",
   "metadata": {},
   "source": [
    "## Full Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f34ef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kafka_topic_modeling/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "kaiser mauer bau sehen dorf wissen leben volk mann heimat\n",
      "\n",
      "Topic #1:\n",
      "offizier reisend verurteilter soldat kommandant hand reisende apparat sehen mann\n",
      "\n",
      "Topic #2:\n",
      "herr diener tür akte gang bleiben scheinen wirt zimmer wirtin\n",
      "\n",
      "Topic #3:\n",
      "klamm wissen wirtin arbeit sehen scheinen herr glauben sagen fragen\n",
      "\n",
      "Topic #4:\n",
      "delamarch fragen lehrer hund mann halten scheinen gehilfe rufen frage\n",
      "\n",
      "Topic #5:\n",
      "advokat kaufmann fragen block hand theater sprechen prozeß wissen pferd\n",
      "\n",
      "Topic #6:\n",
      "welt leben mensch glauben böse herr haus weg alt wissen\n",
      "\n",
      "Topic #7:\n",
      "herr zimmer hand tür vater wissen sehen fragen sagen scheinen\n",
      "\n",
      "Topic #8:\n",
      "herr hand fragen mann tür frau sehen prozeß gericht glauben\n",
      "\n",
      "Topic #9:\n",
      "onkel hand fragen sehen freund delamarch koffer wissen herr scheinen\n"
     ]
    }
   ],
   "source": [
    "from config import DATA_DIR, METADATA_PATH, OUTPUT_DIR, VISUALS_DIR, CSV_DIR, NUM_TOPICS, MAX_DF, MIN_DF\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "for path in [OUTPUT_DIR, VISUALS_DIR, CSV_DIR]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for file in sorted(DATA_DIR.glob(\"*.txt\")):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts.append(f.read())\n",
    "        filenames.append(file.name)\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "except:\n",
    "    print(\"Run: python -m spacy download de_core_news_sm\")\n",
    "    raise\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([\n",
    "        token.lemma_.lower() for token in doc\n",
    "        if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\"]\n",
    "        and not token.is_stop\n",
    "        and token.is_alpha\n",
    "    ])\n",
    "\n",
    "texts_cleaned = [preprocess(text) for text in texts]\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=MAX_DF, min_df=MIN_DF)\n",
    "X = vectorizer.fit_transform(texts_cleaned)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=NUM_TOPICS, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic #{idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-top_n - 1:-1]]))\n",
    "\n",
    "print_topics(lda, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka_topic_modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
